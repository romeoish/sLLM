import streamlit as st
from streamlit_chat import message
import pandas as pd
from embedding_query import embedding_query
from answer_llm import answer_llm
from numpy import dot
from numpy.linalg import norm
from count_token import count_token

st.set_page_config(
page_title="ë†ë¦¼ë´‡",
page_icon=":books:")
provided_data = ""

st.title("ë†ê¸°í‰ x ì†”ë¦¬ë“œì´ì—”ì§€ :red[LLM] :books:")
st.write("í•˜ì´í¼í´ë¡œë°” ê¸°ë°˜ RAGê¸°ìˆ ì„ ì ìš©í•œ ì „ë¬¸LLM(v0.5)")
#st.write("êµ­ê°€ì—°êµ¬ê°œë°œí˜ì‹ ë²• + ì§ˆì˜ì‘ë‹µ ë°ì´í„°")

segments = pd.read_pickle("segment_qna_33726.pkl")
table_phrases = ['í‘œ','ì—‘ì…€','í…Œì´ë¸”','ìì„¸íˆ','ì§§ê²Œ','ê°„ëµ','í•œë¬¸ì¥','ë‘ë¬¸ì¥','ê°„ê²°','í•µì‹¬','ìš”ì•½','ê²°ë¡ ','ì¤‘ìš”í•œ','ê¸¸ê²Œ','ê°„ë‹¨']
table_phrases2 = ['ì•ˆë…•','ëˆ„êµ¬']

#if "conversation" not in st.session_state:
#    st.session_state.conversation = None

if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

if 'messages' not in st.session_state:
        st.session_state['messages'] = [{"role": "assistant", 
                                        "content": "ì•ˆë…•í•˜ì„¸ìš”! ê¶ê¸ˆí•˜ì‹  ê²ƒì´ ìˆìœ¼ë©´ ì–¸ì œë“  ë¬¼ì–´ë´ì£¼ì„¸ìš”!"}]

for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

example_queries = [
    "ì´ê´„ì—°êµ¬ê°œë°œê³„íšì„œì—ì„œ ì´ê´„ì£¼ê´€ì—°êµ¬ê°œë°œê¸°ê´€ê³¼ ì´ê´„ì—°êµ¬ì±…ì„ìë€?",
    "ê³µë™ì—°êµ¬ê°œë°œê¸°ê´€ì—ì„œ ì—°êµ¬ê°œë°œê³¼ì œì˜ ì¼ë¶€ë¥¼ ìœ„íƒí•˜ëŠ” ê²ƒì´ ê°€ëŠ¥í•œì§€?",
    "ê°™ì€ ì—°êµ¬ê°œë°œê¸°ê´€ì´ í•˜ë‚˜ì˜ ì—°êµ¬ê°œë°œê³¼ì œì—ì„œ ë‘ ê°œ ì´ìƒì˜ ê³µë™(ìœ„íƒ)ì—°êµ¬ê°œë°œê¸°ê´€ìœ¼ë¡œ ë™ì‹œì— ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ”ì§€?"
]

#if "button_clicked" not in st.session_state:
#    st.session_state.button_clicked = False

#show_example_queries = not st.session_state.button_clicked

# Display example queries as buttons
if "chat_history" in st.session_state and not st.session_state.chat_history:
    for example_query in example_queries:
        icon = "ğŸ“‹" 
        if st.button(f"{icon} {example_query}", key=example_query, help=example_query):
            st.session_state.messages.append({"role": "user", "content": example_query})
            with st.chat_message("user"):
                st.markdown(example_query)
            with st.chat_message("assistant"): 
                with st.spinner("ë‹µë³€ì„ ìƒê°ì¤‘ì´ì—ìš”..."):
                    preset_text = example_query
                    query_emb = embedding_query(preset_text)

                    top_2 = []
                    for i in range(len(segments)):
                        a = segments['vector'][i]
                        b = query_emb
                        cos_sim = dot(a, b)/(norm(a)*norm(b))
                        top_2.append(cos_sim)

                    if max(top_2) >= 0.975:
                        top_2 = sorted(range(len(top_2)), key=lambda i: top_2[i])[-2:]
                        provided_data = "[ì •ë³´]: " + segments['origin'][top_2[1]] + segments['origin'][top_2[0]]
                    
                    if provided_data == "":
                        if any(phrase in query for phrase in table_phrases):
                            if 'chat_history' in st.session_state:
                                for interaction in st.session_state['chat_history']:
                                    provided_data = "[ì •ë³´]: " + interaction['bot_answer']
                                    break

                    user_text = provided_data + "\n\n" + preset_text
                    answer = answer_llm(user_text)
                    token = count_token(preset_text)

                    current_interaction = {'user_input': example_query, 'bot_answer': answer}
                    st.session_state['chat_history'].insert(0,current_interaction)

                    st.markdown(answer)
                    if max(top_2) >= 0.975:
                        with st.expander("ì°¸ê³  ë¬¸ì„œ í™•ì¸"):
                            st.markdown(f"ì°¸ì¡°í•œ ì •ë³´: â‘ {segments['title'][top_2[1]]} {segments['page_num'][top_2[1]]}pg")
                            st.markdown(f"ì •ë³´ë‚´ìš©:â‘ {segments['origin'][top_2[1]]}")
                            st.markdown(f"ì°¸ì¡°í•œ ì •ë³´: â‘¡{segments['title'][top_2[0]]} {segments['page_num'][top_2[0]]}pg")
                            st.markdown(f"ì •ë³´ë‚´ìš©:â‘¡{segments['origin'][top_2[0]]}")
                        
            st.session_state.messages.append({"role": "assistant", "content": answer})
    

####original query######
if query := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”."):
    st.session_state.messages.append({"role": "user", "content": query})

    with st.chat_message("user"):
        st.markdown(query)
    
    with st.chat_message("assistant"):
        #chain = st.session_state.conversation
            
        with st.spinner("ë‹µë³€ì„ ìƒê°ì¤‘ì´ì—ìš”..."):
            preset_text = query
            query_emb = embedding_query(preset_text)

            top_2 = []
            for i in range(len(segments)):
                a = segments['vector'][i]
                b = query_emb
                cos_sim = dot(a, b)/(norm(a)*norm(b))
                top_2.append(cos_sim)

            if max(top_2) >= 0.975:
                top_2 = sorted(range(len(top_2)), key=lambda i: top_2[i])[-2:]
                provided_data = "[ì •ë³´]: " + segments['origin'][top_2[1]] + segments['origin'][top_2[0]]
                user_text = provided_data + "\n\n" + "[ì§ˆë¬¸]: " +preset_text
                answer = answer_llm(user_text)
                
            if provided_data == "":
                if any(phrase in query for phrase in table_phrases):
                    if 'chat_history' in st.session_state:
                        for interaction in st.session_state['chat_history']:
                            provided_data = "[ì •ë³´]: " + interaction['bot_answer']
                            break
                    user_text = provided_data + "\n\n" + "[ì§ˆë¬¸]: " +preset_text
                    answer = answer_llm(user_text)
                elif any(phrase in query for phrase in ["ì•ˆë…•","ì´ë¦„","ë„ˆ","ë‚˜"]):
                    answer = answer_llm(preset_text)
                else:
                    answer = "í•™ìŠµëœ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤."

            
            #user_text = provided_data + "\n\n" + "[ì§ˆë¬¸]: " +preset_text
            #answer = answer_llm(user_text)
            
            current_interaction = {'user_input': query, 'bot_answer': answer}
            st.session_state['chat_history'].insert(0,current_interaction)

            st.markdown(answer)
            if max(top_2) >= 0.975:
                with st.expander("ì°¸ê³  ë¬¸ì„œ í™•ì¸"):
                    st.markdown(f"ì°¸ì¡°í•œ ì •ë³´: â‘ {segments['title'][top_2[1]]} {segments['page_num'][top_2[1]]}pg")
                    st.markdown(f"ì •ë³´ë‚´ìš©:â‘ {segments['origin'][top_2[1]]}")
                    st.markdown(f"ì°¸ì¡°í•œ ì •ë³´: â‘¡{segments['title'][top_2[0]]} {segments['page_num'][top_2[0]]}pg")
                    st.markdown(f"ì •ë³´ë‚´ìš©:â‘¡{segments['origin'][top_2[0]]}")
                 
    st.session_state.messages.append({"role": "assistant", "content": answer})


##v0.3: ì•ˆë…•,ë“± generalí•œ queryì— ëŒ€í•œ threshold ì¡°ì •
##v0.4: reference ë° ë‚´ìš© ë°˜ì˜, enterëˆ„ë¥¼ ì‹œ prompt clear, êµ­ê°€ì—°êµ¬ê°œë°œí˜ì‹ ë²•(33726í˜¸)